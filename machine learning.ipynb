{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 机器学习笔记"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、基本原理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.预测结果判断  \n",
    "过拟合：部分噪音过分影响了模型  \n",
    "欠拟合：机器学习模型选择有问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.偏差与方差\n",
    "Bias=E $\\widehat{f(x)}$-$\\ f(x)$  \n",
    "偏差衡量的是预测值与真实值的平均偏离程度：   \n",
    "Var=E[$\\widehat{f(x)}$-E $\\widehat{f(x)}$]<sup>2</sup>  \n",
    "方差衡量模型的稳定性  \n",
    "**过拟合**：低偏差、高方差  \n",
    "**欠拟合**：高偏差、低方差  \n",
    "存在最优模型复杂程度（用**均方误差**度量）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.正则化  \n",
    "OLS对训练数据进行过度拟合，可对其回归参数进行惩罚，思路即为**正则化**(Regularization)  \n",
    "**Ridge回归**：$\\Sigma$(y<sub>i</sub>-X<sub>i</sub><sup>T</sup>$\\beta$)<sup>2</sup>+$\\lambda\\Sigma\\beta$<sub>i</sub><sup>2</sup>  \n",
    "**Lasso回归**：$\\Sigma$(y<sub>i</sub>-X<sub>i</sub><sup>T</sup>$\\beta$)<sup>2</sup>+$\\lambda\\Sigma$|$\\beta$<sub>i</sub>|  \n",
    "梯度下降求损失最小  \n",
    "牺牲无偏性，提高泛化能力"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.集成算法\n",
    "原始数据集通过随机采样，形成多个训练样本，分别进行分类训练，各弱分类器通过投票得出结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.深度学习\n",
    "通过多层嵌套，叠加非线性映射"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、决策树模型\n",
    "**决策树**是一种非参数的有监督学习方法，能够从有特征和标签的数据中总结出决策规律，并用树状图的结构来呈现，可以解决**分类**与**回归**问题。本质是一种图结构，需要一系列问题就可以对数据进行分类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.节点\n",
    "根节点：没有进边，只有出边。包含最初的、针对特征的提问。  \n",
    "中间节点：有进边（一条），有出边（若干）。  \n",
    "叶节点：只有进边，没有出边。每个叶节点都是一个类别标签（分类结果）。  \n",
    "子节点和父节点：在相连的两个节点中，更接近根的为父，更接近叶的为子。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.核心问题\n",
    "#### 如何找出最佳的节点和分枝？\n",
    "**信息熵(Entropy)** 用于度量样本集合纯度。  \n",
    "假定样本集合D中第k类样本所占的比例为p<sub>k</sub>（k=1,2,...,|$\\gamma$|），则信息熵定义为：  \n",
    "**Ent(D)=-$\\Sigma$ p<sub>k</sub>log<sub>2</sub>p<sub>k</sub>**  \n",
    "约定：p=0时，Ent=0；Ent(D)最小值为0，最大值为log<sub>2</sub>|$\\gamma$|，信息熵越小，则D的纯度越高。  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### （1）信息增益\n",
    "假定离散属性a有V个可能的取值，使用a对D划分可产生V个分支节点，其中第v个分支节点包含了D中所有在a上取值为a<sup>v</sup>的样本，记作D<sup>v</sup>，并可计算出信息熵Ent(D<sup>v</sup>)。  \n",
    "由于各分支样本数不同，以样本数作为权重加权计算|D<sup>v</sup>|/|D|，可计算信息增益：  \n",
    "**Gain(D,a)=Ent(D)-$\\Sigma$ Ent(D<sup>v</sup>)*|D<sup>v</sup>|/|D|**  \n",
    "**ID3**算法就是以信息增益作为准则选择划分属性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### （2）增益率  \n",
    "由于信息增益准则对于可取值数目较多的属性有所偏好（例如编号），模型泛化能力较差，**C4.5决策树算法**则使用增益率来选择最优划分属性，增益率：  \n",
    "**Gain_ratio(D,a)=Gain(D,a)/IV(a)**  \n",
    "其中：  \n",
    "IV(a)=-$\\Sigma$|D<sup>v</sup>|/|D|*log<sub>2</sub>|D<sup>v</sup>|/|D|  \n",
    "IV(a)为属性a的固有值，取值数目越多，IV(a)通常越大。\n",
    "但增益率对可取值数目较少的属性有所偏好，算法并不直接采用增益率进行选择。  \n",
    "而是**选取信息增益高于平均值的属性，再从中选取增益率最高的属性**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### （3）基尼指数\n",
    "**CART决策树**使用基尼指数（**Gini index**）度量D的纯度：  \n",
    "Gini(D)=1-$\\Sigma$ p<sub>k</sub><sup>2</sup>  \n",
    "Gini(D)反映了从数据集D中随机抽取两个样本，其类别标记不同的概率。故基尼值越小，D的纯度越高。  \n",
    "属性a的**基尼指数**为：\n",
    "Gini_index(D,a)=$\\Sigma$ Gini(D<sup>v</sup>)*|D<sup>v</sup>|/|D|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 如何处理过拟合问题？——剪枝处理 \n",
    "在决策树中，为尽可能正确分类划分训练样本，节点划分过程不断重复，有时造成决策树分枝过多，可能出现过拟合问题。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### （1）预剪枝  \n",
    "在决策树生成过程中，对每个节点在划分前先进行估计，若当前节点的划分不能带来模型泛化能力的性能提升，则停止划分并将当前节点作为叶节点。  \n",
    "预剪枝使得决策树很多分枝未展开，**一方面**：不仅降低了过拟合的风险，还减少了模型的训练时间和测试时间；  \n",
    "**另一方面**：某些分枝当前虽然可能降低模型的泛化能力，但基于此基础的后续分枝可能提高模型性能，预剪枝带来欠拟合风险。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### （2）后剪枝  \n",
    "先从训练集生成完整的决策树，从底向上对非叶节点进行考察，若将该节点替换为叶节点可以提升模型的泛化能力，则替换为叶节点。  \n",
    "后剪枝相对于预剪枝保留了更多的分枝，**一方面**：泛化能力更好，欠拟合风险小；  \n",
    "**另一方面**：生成完全决策树之后进行，并且自底向上注意考察，训练时间与调试时间远大于预剪枝。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
